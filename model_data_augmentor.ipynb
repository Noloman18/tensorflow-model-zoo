{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "from xml.dom import minidom\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASCAL_VOC_ANNOTATIONS_PATH = 'D:/university/datasets/Kangaroo/kangaroo/newAnnots'\n",
    "PASCAL_VOC_TARGET_ANNOTATIONS_PATH = 'D:/university/datasets/Kangaroo/kangaroo/augmentedAnnotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = 'research/object_detection/data/mscoco_complete_label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_MODEL = 'ssd_mobilenet_v1_coco_2017_11_17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pascal_voc_annotations_path():\n",
    "    if not os.path.exists(PASCAL_VOC_ANNOTATIONS_PATH):\n",
    "        raise Exception('The directory doesn''t exist')\n",
    "    \n",
    "    if len(os.listdir(PASCAL_VOC_ANNOTATIONS_PATH)) == 0:\n",
    "        raise Exception('No files were found in {}'.format(PASCAL_VOC_ANNOTATIONS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doesnt redownload the file when already downloaded. Which is fantastic.\n",
    "'''\n",
    "def validate_and_load_existing_model():\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = CHOSEN_MODEL + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=CHOSEN_MODEL, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "  print(model_dir)\n",
    "\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def load_annotation_paths():\n",
    "    annotation_paths = list(pathlib.Path(PASCAL_VOC_ANNOTATIONS_PATH).glob(\"*.xml\"))\n",
    "    shuffle(annotation_paths)\n",
    "    return annotation_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pascal_voc_annotations_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_ANNOTATIONS = load_annotation_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Letlhogonolo Segoe\\.keras\\datasets\\ssd_mobilenet_v1_coco_2017_11_17\\saved_model\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "MODEL = validate_and_load_existing_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_annotation(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_from_annotation(annotation: Element):\n",
    "    return os.path.join(annotation.find('path').text, annotation.find('filename').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bounding_boxes(annotation: Element):\n",
    "    bndboxes = []\n",
    "    for member_object in annotation.findall('object'):\n",
    "        bnd_box = member_object.find('bndbox')\n",
    "        xmin = int(bnd_box.find('xmin').text)\n",
    "        ymin = int(bnd_box.find('ymin').text)\n",
    "        xmax = int(bnd_box.find('xmax').text)\n",
    "        ymax = int(bnd_box.find('ymax').text)\n",
    "        \n",
    "        ith_bndbox = {\n",
    "            'xmin':xmin,\n",
    "            'ymin':ymin,\n",
    "            'xmax':xmax,\n",
    "            'ymax':ymax,\n",
    "            'area':(xmax- xmin)*(ymax - ymin)\n",
    "        }\n",
    "        bndboxes.append(ith_bndbox)\n",
    "    return bndboxes            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_size(annotation: Element):\n",
    "    size_element = annotation.find('size')\n",
    "    height = int(size_element.find('height').text)\n",
    "    width = int(size_element.find('width').text)\n",
    "    return {\n",
    "        'width':width,\n",
    "        'height':height,\n",
    "        'area':width*height\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = model(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, image_path, min_score=0.5):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "  count = np.sum(output_dict['detection_scores']>min_score)\n",
    "  return {\n",
    "      'num_detections':count, \n",
    "      'detection_boxes':output_dict['detection_boxes'][0:count], \n",
    "      'detection_classes': output_dict['detection_classes'][0:count]}\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(model, image_path):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "  # Visualization of the results of a detection.\n",
    "  category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "\n",
    "  display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bndbox_dict(annotation, bnd_box_as_array):\n",
    "    size = get_size(annotation)\n",
    "    xmin = int(bnd_box_as_array[1]*size['width'])\n",
    "    ymin = int(bnd_box_as_array[0]*size['height'])\n",
    "    xmax = int(bnd_box_as_array[3]*size['width'])\n",
    "    ymax = int(bnd_box_as_array[2]*size['height'])\n",
    "    \n",
    "    return {\n",
    "        'xmin':xmin,\n",
    "        'ymin':ymin,\n",
    "        'xmax':xmax,\n",
    "        'ymax':ymax,\n",
    "        'area':(xmax-xmin)*(ymax-ymin)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_keep_inferrence(annotated_boxes, inferred_box, cutoff_area=0.6):\n",
    "    for annotated_box in annotated_boxes:\n",
    "        x1 = max(annotated_box['xmin'], inferred_box['xmin'])\n",
    "        y1 = max(annotated_box['ymin'], inferred_box['ymin'])\n",
    "        x2 = min(annotated_box['xmax'], inferred_box['xmax'])\n",
    "        y2 = min(annotated_box['ymax'], inferred_box['ymax'])\n",
    "        \n",
    "        min_area = min(annotated_box['area'], inferred_box['area'])\n",
    "        intersection = (x2-x1)*(y2-y1)\n",
    "        intersection_area = float(intersection)/min_area\n",
    "        if intersection_area > cutoff_area:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_element(element: Element, key: str, val):\n",
    "    sub_element = ET.SubElement(element, key)\n",
    "    sub_element.text = str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (<ipython-input-84-db50902b856a>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-84-db50902b856a>\"\u001B[1;36m, line \u001B[1;32m26\u001B[0m\n\u001B[1;33m    output_file = os.path.join(PASCAL_VOC_TARGET_ANNOTATIONS_PATH, file_name)\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PASCAL_VOC_TARGET_ANNOTATIONS_PATH):\n",
    "    os.makedirs(PASCAL_VOC_TARGET_ANNOTATIONS_PATH)\n",
    "\n",
    "annotation_length = len(LIST_OF_ANNOTATIONS)\n",
    "for j in range(annotation_length):\n",
    "    annotation_name = LIST_OF_ANNOTATIONS[j]\n",
    "    parent_dir, file_name = os.path.split(annotation_name)\n",
    "    myannotation = load_annotation(annotation_name)\n",
    "    bounding_boxes = get_bounding_boxes(myannotation)\n",
    "    image_path = get_image_from_annotation(myannotation)\n",
    "    try:\n",
    "        output_dict = perform_inference(MODEL, image_path)\n",
    "        for i in range(output_dict['num_detections']):\n",
    "            inferred_box = convert_to_bndbox_dict(myannotation, output_dict['detection_boxes'][i])\n",
    "            if should_keep_inferrence(bounding_boxes, inferred_box):\n",
    "                new_object = ET.SubElement(myannotation,'object')\n",
    "                add_text_to_element(new_object, 'pose','Unspecified')\n",
    "                add_text_to_element(new_object,'name',LABEL_MAP[output_dict['detection_classes'][i]]['name'])\n",
    "                new_bnding_box = ET.SubElement(new_object,'bndbox')\n",
    "                add_text_to_element(new_bnding_box, 'xmax',inferred_box['xmax'])\n",
    "                add_text_to_element(new_bnding_box, 'xmin',inferred_box['xmin'])\n",
    "                add_text_to_element(new_bnding_box, 'ymax',inferred_box['ymax'])\n",
    "                add_text_to_element(new_bnding_box, 'ymin',inferred_box['ymin'])\n",
    "        output_file = os.path.join(PASCAL_VOC_TARGET_ANNOTATIONS_PATH, file_name)\n",
    "\n",
    "        with open(output_file, 'tw') as f:\n",
    "            txt = ET.tostring(myannotation, 'utf-8').decode('utf-8').replace('\\n', '')\n",
    "            txt = re.sub(r\"\\s{3,}\",\"\",txt)\n",
    "            f.write(minidom.parseString(txt.encode('utf-8')).toprettyxml(indent=\"   \"))\n",
    "            print('Finished outputting',output_file,'Using image',image_path, 100*float(j)/annotation_length, 'Complete')\n",
    "\n",
    "        #show_inference(MODEL, image_path)\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "        print('Failed to process',annotation_name, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}